# 新闻数据采集器

<cite>
**本文档引用的文件**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py)
- [config.py](file://backend_core/config/config.py)
</cite>

## 目录
1. [简介](#简介)
2. [项目结构](#项目结构)
3. [核心组件](#核心组件)
4. [架构概述](#架构概述)
5. [详细组件分析](#详细组件分析)
6. [依赖分析](#依赖分析)
7. [性能考虑](#性能考虑)
8. [故障排除指南](#故障排除指南)
9. [结论](#结论)

## 简介
本文档全面记录了新闻数据采集器的实现细节，重点分析`NewsCollector`类的整体架构、初始化流程、市场新闻采集、新闻分类与处理、数据库操作以及定时任务调度策略。系统基于akshare库从财新网等来源采集市场新闻，并通过结构化处理、分类、摘要提取和标签生成，实现高质量新闻数据的自动化采集与管理。

## 项目结构
新闻采集功能主要分布在`backend_core/data_collectors/`和`backend_core/schedulers/`目录下，采用模块化设计，分离数据采集逻辑与调度逻辑，确保系统的可维护性和扩展性。

```mermaid
graph TB
subgraph "数据采集模块"
NC[news_collector.py]
Base[base.py]
end
subgraph "调度模块"
NS[news_scheduler.py]
end
NC --> NS : "提供采集接口"
NS --> NC : "调用采集任务"
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L50)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L1-L30)

**本节来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L50)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L1-L30)

## 核心组件
`NewsCollector`类是新闻采集系统的核心，负责从akshare获取新闻数据、进行结构化处理、分类、摘要提取、标签生成，并将结果保存至数据库。同时提供热门资讯更新和旧新闻清理功能。

**本节来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L25-L100)

## 架构概述
系统采用分层架构，分为数据采集层、数据处理层、数据存储层和任务调度层。`NewsCollector`类封装了从数据获取到存储的完整流程，`news_scheduler.py`通过`schedule`库实现定时任务调度。

```mermaid
graph TD
A[定时任务调度] --> B[新闻采集器]
B --> C[akshare数据源]
B --> D[数据库]
B --> E[日志系统]
A --> F[每日清理任务]
A --> G[每小时热门更新]
style A fill:#f9f,stroke:#333
style B fill:#bbf,stroke:#333
style C fill:#9f9,stroke:#333
style D fill:#f96,stroke:#333
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L50)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L1-L50)

## 详细组件分析

### NewsCollector 类分析
`NewsCollector`类实现了完整的新闻采集与处理流程，包含初始化、新闻采集、分类、摘要提取、标签生成、数据库操作等核心功能。

#### 初始化流程
```mermaid
classDiagram
class NewsCollector {
+session : SessionLocal
+__init__()
+collect_market_news() List[Dict]
+collect_stock_news(stock_code) List[Dict]
+_classify_news(title, content) int
+_extract_summary(content) str
+_extract_tags(title, content) List[str]
+save_news_to_db(news_list) int
+update_hot_news() bool
+cleanup_old_news(days) int
+close()
}
NewsCollector --> SessionLocal : "依赖"
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L15-L40)

#### 市场新闻采集流程
```mermaid
sequenceDiagram
participant Scheduler as "news_scheduler"
participant Collector as "NewsCollector"
participant Akshare as "akshare"
participant DB as "数据库"
Scheduler->>Collector : collect_and_save_market_news()
Collector->>Akshare : ak.stock_news_main_cx()
Akshare-->>Collector : 返回新闻DataFrame
Collector->>Collector : 遍历处理每条新闻
Collector->>Collector : _classify_news()
Collector->>Collector : _extract_summary()
Collector->>Collector : _extract_tags()
Collector->>DB : save_news_to_db()
DB-->>Collector : 返回保存数量
Collector->>Scheduler : 返回采集结果
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L50-L150)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L30-L50)

#### 新闻处理算法
```mermaid
flowchart TD
Start([开始处理新闻]) --> Classify["分类: _classify_news()"]
Classify --> Summary["摘要: _extract_summary()"]
Summary --> Tags["标签: _extract_tags()"]
Tags --> Store["保存到数据库"]
subgraph 分类逻辑
Classify --> Policy{"包含政策关键词?"}
Policy --> |是| Cat3["category_id = 3"]
Policy --> |否| Company{"包含公司关键词?"}
Company --> |是| Cat4["category_id = 4"]
Company --> |否| International{"包含国际关键词?"}
International --> |是| Cat5["category_id = 5"]
International --> |否| Analysis{"包含分析关键词?"}
Analysis --> |是| Cat6["category_id = 6"]
Analysis --> |否| Cat2["category_id = 2"]
end
subgraph 摘要提取
Summary --> Length{"内容≤200字符?"}
Length --> |是| Return["直接返回"]
Length --> |否| Sentence["按句号分割"]
Sentence --> Loop["逐句累加"]
Loop --> |≤200| Add["添加句子"]
Loop --> |>200| Truncate["截断并加..."]
end
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L200-L350)

### 数据库操作分析
`save_news_to_db()`方法实现了新闻数据的去重插入机制，通过标题和发布时间的组合判断是否为重复新闻，确保数据的唯一性。

```mermaid
flowchart TD
A[开始保存新闻列表] --> B{列表为空?}
B --> |是| C[返回0]
B --> |否| D[遍历每条新闻]
D --> E["检查重复: SELECT WHERE title=publish_time"]
E --> F{已存在?}
F --> |是| G[跳过]
F --> |否| H["INSERT 新闻数据"]
H --> I[计数+1]
I --> J{处理下一条?}
J --> |是| D
J --> |否| K[提交事务]
K --> L[返回保存数量]
K --> M{异常?}
M --> |是| N[回滚事务]
N --> O[记录错误]
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L350-L400)

**本节来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L428)

## 依赖分析
系统主要依赖akshare库获取新闻数据，SQLAlchemy进行数据库操作，schedule库实现定时任务。`NewsCollector`类依赖数据库会话，`news_scheduler`依赖`NewsCollector`提供的接口。

```mermaid
graph TD
NS[news_scheduler] --> NC[NewsCollector]
NC --> AK[akshare]
NC --> DB[SQLAlchemy]
NC --> LOG[logging]
NS --> SCHEDULE[schedule]
style NS fill:#f96,stroke:#333
style NC fill:#69f,stroke:#333
style AK fill:#9f9,stroke:#333
style DB fill:#f96,stroke:#333
```

**图示来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L20)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L1-L20)

**本节来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L1-L50)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L1-L50)

## 性能考虑
系统在性能方面进行了多项优化：批量处理新闻数据、使用数据库事务、合理的异常处理机制、定时任务的合理调度间隔（30分钟采集、1小时更新、每日清理），避免对源站和数据库造成过大压力。

## 故障排除指南
常见问题包括akshare连接失败、数据库连接异常、新闻重复采集等。系统通过完善的日志记录（INFO、ERROR级别）和异常捕获机制，便于问题定位。建议检查网络连接、数据库状态、akshare服务可用性。

**本节来源**
- [news_collector.py](file://backend_core/data_collectors/news_collector.py#L100-L200)
- [news_scheduler.py](file://backend_core/schedulers/news_scheduler.py#L80-L110)

## 结论
新闻数据采集器实现了从财新网等来源自动化采集市场新闻的完整流程，具备新闻分类、摘要提取、标签生成、去重存储、热门更新和旧新闻清理等完整功能。通过`news_scheduler`的定时任务配置，实现了市场新闻采集、热门资讯更新和旧新闻清理的自动化调度，为股票分析系统提供了稳定可靠的新闻数据支持。